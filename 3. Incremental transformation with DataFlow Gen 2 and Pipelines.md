# Incremental transformation with DataFlow Gen 2 and Pipelines

Dataflows are familiar to Power BI users, as they are based on Power Query. Gen 2 dataflows are particularly interesting as they do not have to store their data in a proprietary manner as do Gen 1 dataflows. Instead, they can output to lakehouses, KQL databases, and more. In this module, we'll use a Gen 2 dataflow to transform a thermostat file before using it to update lakehouse data with a pipeline.

## Transforming the file

In the previous two exercises, we started with clean data. In the real world, these files arrive with some extra formatting that needs to be dealt with before loading.

Navigate to the "Fabric Workshop xxx" workspace, and open the "Lakehouse_Fabxxx" lakehouse. Open the Files node, and add a new folder named "Processing". Navigate to this folder and upload the file "2023 - October.csv" that you downloaded in the first exercise.

![](assets/20240319_141017_image.png)

Navigate back to the "Fabric Workshop xxx" workspace, select the "New" button from the ribbon, and select "More options". Next, select "Dataflow Gen 2". It may take a moment to respond to the click. If it takes too long, refresh your browser window.

Although our data is in a CSV file, it is not stored locally, or in SharePoint, so do NOT select the "Import from a Text/CSV file" option on the main screen. Instead, select the "Get data" button from the ribbon, and then click on the Recommended" tab in the OneLake data hub section. Select your lakehouse from the list. 

![](assets/20240319_142318_image.png)

Expand the "Files" node. Select the "Processing" folder, then click on the "Create" button. 

Select the Content column. Change its data type to "Binary" by clicking on the icon on the left side of the header, and selecting "Binary".

![](assets/20240319_142741_image.png)

Click the "Combine" icon on the right side of the Content column header, or select "Combine - Combine files" from the ribbon. Click "OK" on the dialog box that pops up. This will build a function that will combine all files in the folder.

Note that we don't have proper column names or data types. We need to perform some Power Query magic to make this work. By making changes to the transform sample file, those changes will be repeated for every file that is subsequently combined. 

Select the "Transform sample file" node in the "Queries" pane.

Click on the table cell in the upper left hand corner of the table. Select "Remove top rows". Enter 5 for the Number of rows, and click "OK".

Click on the same table icon, and select "Use first row as headers".

![](assets/20240319_143301_image.png)

Select the "Processing" query in the "Queries" pane. Click the "Refresh" button in the ribbon to reprocess the files. You will see an error. This is due to the fact that we changed the column names for the sample file, so we need to set their types manually.

Delete the last step in the "Applied steps" window named "Changed column type 1". You will see your data again. Set the columns to their correct types as below (use the column header type icon, or the Data type button in the ribbon). Note that lakehouses do not support the Time data type. We will be outputting to a lakehouse delta table, so we need to change it to Date/Time instead.

| Column                     | Type           |
| -------------------------- | -------------- |
| Date                       | Date           |
| Time                       | Date/Time      |
| System Setting             | Text           |
| System Mode                | Text           |
| Calendar Event             | Text           |
| Program Mode               | Text           |
| Cool Set Temp (C)          | Decimal number |
| Heat Set Temp (C)          | Decimal number |
| Current Temp               | Decimal number |
| Current Humidity (%RH)     | Whole number   |
| Outdoor Temp (C)           | Decimal number |
| Wind Speed (km/h)          | Whole number   |
| Cool Stage 1 (sec)         | Whole number   |
| Cool Stage 2 (sec)         | Whole number   |
| Heat Stage 1 (sec)         | Whole number   |
| Heat Stage 2 (sec)         | Whole number   |
| Aux Heat 1 (sec)           | Whole number   |
| Fan (sec)                  | Whole number   |
| DM Offset                  | Decimal number |
| Thermostat Temperature (C) | Decimal number |
| Thermostat Humidity (%RH)  | Whole number   |
| Thermostat Motion          | Whole number   |

![](assets/20240319_144237_image.png)

## Set the destination

Select the "+" icon beside "Data destination" from the bottom right part of the screen.

Select "Lakehouse".

Leave the "Lakehouse" connection selected, and select "Next".

Select the "Existing table" radio button. Open the Lakehouse node, then the "Fabric Workshop xxx" node and finally the ""Lakehouse_Fabxxx" node. You should see the "thermostat_readings" table from the previous two tutorials. Select the table and click the "Next" button.

![](assets/20240319_144813_image.png)

**IMPORTANT** **- select the "Append" option. We will be adding data, not replacing it.**

The column names don't match up perfectly (in the earlier exercise they were modified to avoid spaces). Select the correctcorresponding column for each destination field.

![](assets/20240319_145134_image.png)

When ready, click the "Save settings" button on the bottom right.

Select the "Dataflow 1" dropdown in the upper left corner. Change the name to "Get new files".

Click the "Publish" button on the bottom right to publish your dataflow.

The dataflow will now run. When it is complete, the "Refreshed" value will be updated in the workspace. THis may take a minute or two. 

Open the "Thermostat report 1" report that you completed in an earlier exercise. Note that October data is now included. This is because the dataflow pushed the data from October into the delta table, and Power BI is using DirectLake mode to connect to the data so no refresh is required. Drill up on the chart to get a monthly view.

Navigate to the Processing folder in your lakehouse and delete the file 2023 - October.csv.

## Automate file processing with a pipeline

Dataflows can be scheduled, but in our case, there is no point in re-adding the same data over and again. Pipelines give us the ability to not only load data, but also to manipulate files. We'll build a pipeline to retrieve new files, load them to the lakehouse with the dataflow created above, and then archive those same files.

Navigate to the "Fabric Workshop xxx" workspace, and open the "Lakehouse_Fabxxx" lakehouse. Open the Files node, then add a new folder called "Input" and another folder called "Archives". Navigate to the new "Input" folder, and upload the file "2023 - November.csv" that you downloaded in the first exercise.

Navigate back to the "Fabric Workshop xxx" workspace, select the "New" button from the ribbon, and select "More options". Next, select "Data pipeline"**.** Name it "Thermostat file processing"

Select the "Copy data" tile on the main page.

Scroll down to the "Data sources" section, select the "Workspace" tab, then select "Lakehouse".

Choose "Lakehouse_Fabxxx" from the Lakehouse dropdown, and click "Next"

![](assets/20240319_150226_image.png)

Select the "Files" radio button.

Select the "Input" folder, then the "Schema agnostic (binary copy)" checkbox. This allows the pipeline to work with files themselves, not the contents of the files. Click "Next".

![](assets/20240319_150331_image.png)

Select "Lakehouse" as the data destination, and again select "Lakehouse_Fabxxx" from the dropdown. Click "Next".

Click Browse for the folder path, and select the "Processing" folder. Click "OK". Select "Preserve hierarchy" for the Copy behavior. Click "Next".


![](assets/20240319_150542_image.png)

Leave the File format as "Binary" and click "Next".

Deselect "Start data transfer immediately" and click "OK".

![](assets/20240319_150630_image.png)

Select the "Copy data" action, then select the "Source" tab. Open the Advanced section and select the "Delete files after completion" checkbox.

![](assets/20240319_150930_image.png)

Select "Dataflow" from the ribbon to add a new dataflow action.

Connect the "On Completion" event from the Copy data activity to the input of the Dataflow activity.

Select the Dataflow activity, and set the name to "Load data".

Click the settings tab, and then select "Get new files" in the Dataflow field.

Add another "Copy data" activity (from the ribbon). Connect the "On success" event from the Dataflow activity to the new Copy data activity.

Select the new "Copy data" activity and configure it identically to the first one, using the "Processing" folder as the source, and the "Archives" folder as the destination.Use the "Browse" button to help select the folders.

![](assets/20240319_151426_image.png)Save the pipeline with the save icon in the ribbon, and then run the pipeline by selecting the "Run" button in the ribbon. Monitor the output in the output tab. You will see each step complete. 

The pipeline will move the source file into the processing folder, run the dataflow to extract the data into the lakehouse, and then archive the file, removing it from the processing folder.

Open the Thermostat report 1 report from the earlier exercise. You should see November's data loaded into it. Navigate to the Files folder in the lakehouse. The November data file should also be in the Archive folder.

This exercise demonstrate how both dataflows and pipelines can be used together or separately for low code, high value ETL operations.
